# -*- coding: utf-8 -*-
"""Desafio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uHR-CFNRuH3RYMyUSY2UXCtSW-cEOMKQ
"""

# Desafio do 5° Dia do Workshop Dados da Fábrica de Software do UNIPÊ
# Arthur Trindade

# 1. Abordar o problema e Analisar

# Qual é o objetivo do modelo?
  # O nosso objetivo é criar um modelo de Machine Learning capaz de classificar
  # o alertlevel (nível de alerta) de um incêndio florestal com base em outras
  # características, como país, duração, e área queimada. Este é um problema de
  # classificação multiclasse, pois a variável alvo possui várias categorias.


# Existe algum desafio especial (dados desbalanceados, missing values, etc.)?
  # Vamos investigar a existência de dados faltantes (missing values) e se as
  # classes do nosso alvo (alertlevel) estão desbalanceadas.

# 2. Obter os dados

  # Carregando os dados sobre OverWatch retirado do Kaggle com a biblioteca pandas.

# Importando bibliotecas
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import joblib

df = pd.read_csv("/content/Forest Fires.csv")

print("2. Amostra Inicial dos Dados")
print(df.head())

# 3. Explorar os dados

print("Informações Gerais do DataFrame")
df.info()

# 4. Tratamento de dados

df['alertlevel'] = df['alertlevel'].str.upper()

df = df[df['alertlevel'] != 'ORANGE']

median_severity = df['severity'].median()
df['alertlevel_binary'] = df['severity'].apply(lambda x: 'HIGH' if x > median_severity else 'LOW')

print(f"\nSeveridade mediana usada como ponto de corte: {median_severity}")
print("Distribuição de nova variável alvo 'alertlevel_binary':")
print(df['alertlevel_binary'].value_counts())

features = ['country', 'severity', 'Duration (days)']
target = 'alertlevel_binary'
df_model = df[features + [target]]

# 5. Separar Base de Dados em Arrays

x_data = df_model.drop(columns=[target]).values
y_data = df_model[target].values
print("Dimensões de x_data:", x_data.shape)
print("Dimensões de y_data:", y_data.shape)

# Passo 6: Técnicas de Pré-processamento

categorical_feature_index = 0

encoder_cat = ColumnTransformer(
    transformers=[('OneHot', OneHotEncoder(handle_unknown='ignore'), [categorical_feature_index])],
    remainder='passthrough'
)

preprocessor = Pipeline(steps=[
    ('Encoder', encoder_cat),
    ('Scaler', StandardScaler(with_mean=False))
])

x_data_processed = preprocessor.fit_transform(x_data)

label_encoder_y = LabelEncoder()
y_data_encoded = label_encoder_y.fit_transform(y_data)
class_names = label_encoder_y.classes_

print("Dimensões de x_data após pré-processamento:", x_data_processed.shape)
print("Classes da variável alvo:", class_names)

# 7. Dividir Base de Dados entre Treino e Teste

x_train, x_test, y_train, y_test = train_test_split(
    x_data_processed, y_data_encoded, test_size=0.2, random_state=42
)

print("Dimensões do conjunto de treino:", x_train.shape)
print("Dimensões do conjunto de teste:", x_test.shape)

# 8. Definir vários modelos e aplicar Treinamento

  # Modelo 1: Regressão

modelLogistic = LogisticRegression(max_iter=1000)
modelLogistic.fit(x_train, y_train)
print("Regressão Logística treinada.")

# 8. Definir vários modelos e aplicar Treinamento

  # Modelo 2: Naives Bayes

modelNB = GaussianNB()
modelNB.fit(x_train.toarray(), y_train)
print("Naive Bayes treinado.")

# 8. Definir vários modelos e aplicar Treinamento

  # Modelo 3: Árvore de Decisão

modelTree = DecisionTreeClassifier(random_state=42)
modelTree.fit(x_train, y_train)
print("Árvore de Decisão treinada.")

# 8. Definir vários modelos e aplicar Treinamento

  # Modelo 4: Random Forest

modelRF = RandomForestClassifier(random_state=42)
modelRF.fit(x_train, y_train)
print("Random Forest treinado.")

# 9. Validar Modelos

def PerformanceMetrics(y_true, predict, class_name_list, model_name):
    accuracy = accuracy_score(y_true, predict)
    cm = confusion_matrix(y_true, predict)

    print(f"\n--- Resultados para o modelo: {model_name} ---")
    print(f"A acurácia geral foi de {accuracy * 100:.2f}%.")

    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=class_name_list, yticklabels=class_name_list)
    plt.title(f"Matriz de Confusão - {model_name}")
    plt.ylabel('Verdadeiro')
    plt.xlabel('Predito')
    plt.show()

# 9. Validar Modelos

  # Modelo 1: Regressão

predictLogistic = modelLogistic.predict(x_test)
PerformanceMetrics(y_test, predictLogistic, class_names, "Regressão Logística")

# 9. Validar Modelos

  # Modelo 2: Naives Bayes

predictNB = modelNB.predict(x_test.toarray())
PerformanceMetrics(y_test, predictNB, class_names, "Naive Bayes")

# 9. Validar Modelos

  # Modelo 3: Árvore de Decisão

predictTree = modelTree.predict(x_test)
PerformanceMetrics(y_test, predictTree, class_names, "Árvore de Decisão")

# 9. Validar Modelos

  # Modelo 4: Random Forest

predictRF = modelRF.predict(x_test)
PerformanceMetrics(y_test, predictRF, class_names, "Random Forest")

# 10. Salvar o melhor modelo

best_model = modelTree

joblib.dump(best_model, "decision_tree_model.pkl")
joblib.dump(preprocessor, "preprocessor.pkl")

print("Melhor modelo (Árvore de Decisão) e preprocessor foram salvos com sucesso!")